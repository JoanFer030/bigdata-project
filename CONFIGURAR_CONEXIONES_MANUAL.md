# ðŸ”§ Configurar Conexiones Manualmente en Airflow

Ya que `airflow_settings.yaml` tiene problemas al cargar las conexiones automÃ¡ticamente, vamos a crearlas manualmente desde la UI de Airflow.

## ðŸ“‹ Instrucciones

### 1. Acceder a Airflow UI

```
URL: http://localhost:8080
Usuario: admin
ContraseÃ±a: admin
```

### 2. Navegar a Conexiones

```
Admin â†’ Connections
```

O directamente:
```
http://localhost:8080/connection/list/
```

---

## âœ… CONEXIÃ“N 1: PostgreSQL

Click en **"+"** (Add a new record)

```
Connection Id:   postgres_datos_externos
Connection Type: Postgres
Host:            postgresql
Schema:          mitma
Login:           admin
Password:        muceim-duckduck.2025!
Port:            5432
```

**Extra:** (dejar vacÃ­o o agregar):
```json
{}
```

Click **Save**

---

## âœ… CONEXIÃ“N 2: RustFS (S3)

Click en **"+"** (Add a new record)

```
Connection Id:   rustfs_s3_conn
Connection Type: Amazon Web Services
```

**Extra:** (copiar exactamente):
```json
{
  "endpoint_url": "http://rustfs:9000",
  "region_name": "us-east-1",
  "aws_access_key_id": "admin",
  "aws_secret_access_key": "muceim-duckduck.2025!"
}
```

**IMPORTANTE:** Dejar Login y Password **vacÃ­os** (las credenciales van en Extra)

Click **Save**

---

## âœ… VARIABLES

Navegar a:
```
Admin â†’ Variables
```

O directamente:
```
http://localhost:8080/variable/list/
```

### Crear estas variables:

Click en **"+"** para cada una:

| Key | Val | 
|-----|-----|
| `RUSTFS_BUCKET` | `mitma` |
| `POSTGRES_DB_NAME` | `mitma` |
| `DATALAKE_MODE` | `production` |

---

## âœ… Verificar Pool

Navegar a:
```
Admin â†’ Pools
```

DeberÃ­a existir:
```
Pool Name: bronze_ingestion_pool
Slots: 3
Description: Pool para limitar tareas de ingesta en capa Bronze
```

Si no existe, crearlo manualmente.

---

## ðŸ§ª Probar Conexiones

### Desde la UI:

1. Ve a Admin â†’ Connections
2. Click en el **icono de lÃ¡piz** de `postgres_datos_externos`
3. Click en **"Test"** al final del formulario
4. DeberÃ­a aparecer: âœ… "Connection successfully tested"

5. Repetir para `rustfs_s3_conn`

### Desde Python (opcional):

Puedes ejecutar el task `verify_connections` en tu DAG `bronze_mitma_all_datasets` para verificar ambas conexiones.

---

## ðŸ“Š Resultado Esperado

DespuÃ©s de configurar todo, deberÃ­as tener:

**Connections (2):**
- âœ… postgres_datos_externos
- âœ… rustfs_s3_conn

**Variables (3):**
- âœ… RUSTFS_BUCKET = mitma
- âœ… POSTGRES_DB_NAME = mitma  
- âœ… DATALAKE_MODE = production

**Pools (1):**
- âœ… bronze_ingestion_pool (slots: 3)

---

## ðŸš€ Siguiente Paso

Una vez configurado todo, ejecuta el DAG:

```
DAG: bronze_mitma_all_datasets
Params:
  start: 2023-01-01
  end: 2023-01-03
```

El primer task (`verify_connections`) validarÃ¡ que todo estÃ© correcto.

---

## ðŸ’¡ Nota

Si ves el error al iniciar Airflow:
```
Error: error adding connections: error listing connections...
```

**Puedes ignorarlo**. Es solo un problema del CLI al leer el YAML, pero Airflow funciona perfectamente. Las conexiones creadas manualmente en la UI funcionarÃ¡n sin problemas.
